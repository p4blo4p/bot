name: ğŸ” URLWatch Monitoring - Detailed Tracking

on:
  schedule:
    # Ejecutar cada 6 horas
    - cron: '0 */6 * * *'
  workflow_dispatch: # Permitir ejecuciÃ³n manual
  
env:
  TZ: Europe/Madrid

jobs:
  monitor:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Necesario para el historial
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: ğŸ“¦ Install URLWatch and dependencies
      run: |
        pip install urlwatch
        pip install beautifulsoup4 lxml requests PyYAML

    - name: ğŸ“ Create directories
      run: |
        mkdir -p logs
        mkdir -p .urlwatch

    - name: ğŸ”§ Setup URLWatch configuration
      run: |
        # Crear configuraciÃ³n detallada
        cat > .urlwatch/config.yaml << 'EOF'
        display:
          new: true
          error: true
          unchanged: false
          empty-diff: true
        
        report:
          text:
            line_length: 120
            details: true
            footer: true
            minimal: false
        
        # ConfiguraciÃ³n para mejor seguimiento
        storage:
          minidb:
            filename: .urlwatch/cache.db
            
        reporters:
          - text:
              filename: logs/detailed_report.txt
              details: true
        EOF

    - name: ğŸ“ Load URLs configuration from repository
      run: |
        # Si existe urls2watch.yaml en el repo, usarlo
        if [ -f "urls2watch.yaml" ]; then
          echo "âœ… Usando urls2watch.yaml existente"
          cp urls2watch.yaml .urlwatch/urls.yaml
        else
          # Crear configuraciÃ³n por defecto
          cat > .urlwatch/urls.yaml << 'EOF'
        name: "Oposiciones UCA - InformÃ¡tica"
        url: "https://personal.uca.es/oposiciones-turno-libre/"
        timeout: 30
        filter:
          - html2text
          - grep: "informÃ¡tica"
          - strip
        ---
        name: "Cursos INAP - InformÃ¡tica"  
        url: "https://buscadorcursos.inap.es/#/?abierto=true&funcion=6"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|sistemas|tecnolog"
          - strip
        ---
        name: "Ayto Puerto Real - TablÃ³n"
        url: "https://puertoreal.sedelectronica.es/board" 
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|tÃ©cnico|sistemas"
          - strip
        ---
        name: "Ayto Puerto Real - OPE"
        url: "https://puertoreal.es/oferta-publica-de-empleo/"
        timeout: 30
        filter:
          - html2text  
          - grep: "(?i)informÃ¡tica|tÃ©cnico|sistemas"
          - strip
        ---
        # URLs corregidas (las anteriores daban 404)
        name: "BOE - BÃºsqueda Oposiciones"
        url: "https://www.boe.es/buscar/doc.php?coleccion=boe&modo=ultimosdias&tm=4"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|sistemas|tecnolog|tÃ©cnico superior|oposiciÃ³n"
          - strip
        ---
        name: "Junta de AndalucÃ­a - FunciÃ³n PÃºblica"
        url: "https://www.juntadeandalucia.es/organismos/funcionpublica/"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|sistemas|tecnolog|oposiciÃ³n"
          - strip
        EOF
        fi

    - name: ğŸ—‚ï¸ Load previous cache if exists
      run: |
        # Si existe cache anterior en el repo, restaurarlo
        if [ -f "cache.db" ]; then
          cp cache.db .urlwatch/cache.db
          echo "âœ… Cache anterior restaurado"
        else
          echo "â„¹ï¸ No hay cache anterior, iniciando desde cero"
        fi

    - name: ğŸ“Š Create improved tracking script
      run: |
        cat > track_changes.py << 'EOF'
        #!/usr/bin/env python3
        import sqlite3
        import json
        import os
        import yaml
        from datetime import datetime
        from pathlib import Path

        def analyze_cache_changes():
            """Analiza la cache y genera reporte de cambios"""
            
            cache_file = Path(".urlwatch/cache.db")
            if not cache_file.exists():
                print("âŒ No se encontrÃ³ archivo de cache")
                return
            
            # Cargar URLs desde el archivo de configuraciÃ³n
            urls_config = {}
            try:
                with open(".urlwatch/urls.yaml", 'r') as f:
                    urls_data = yaml.safe_load(f)
                    if isinstance(urls_data, list):
                        for item in urls_data:
                            if isinstance(item, dict) and 'name' in item and 'url' in item:
                                urls_config[item['url']] = item['name']
            except Exception as e:
                print(f"âš ï¸ Error cargando configuraciÃ³n de URLs: {e}")
            
            try:
                conn = sqlite3.connect(str(cache_file))
                cursor = conn.cursor()
                
                # Obtener todas las entradas ordenadas por timestamp
                cursor.execute("""
                    SELECT guid, timestamp, tries, etag, 
                           CASE WHEN data IS NULL THEN 0 ELSE 1 END as has_data,
                           length(data) as data_length
                    FROM CacheEntry 
                    ORDER BY timestamp DESC
                """)
                
                results = cursor.fetchall()
                current_time = datetime.now()
                
                # Procesar resultados
                sites_info = {}
                for row in results:
                    guid, timestamp, tries, etag, has_data, data_length = row
                    
                    if guid not in sites_info:
                        sites_info[guid] = {
                            'latest_timestamp': timestamp,
                            'latest_tries': tries,
                            'has_data': has_data,
                            'data_length': data_length or 0,
                            'entries_count': 0
                        }
                    
                    sites_info[guid]['entries_count'] += 1
                
                # Generar reporte detallado
                report_data = {
                    'generated_at': current_time.isoformat(),
                    'generated_readable': current_time.strftime("%d/%m/%Y %H:%M:%S"),
                    'sites': {}
                }
                
                # Por cada GUID, intentar encontrar la URL correspondiente
                cursor.execute("SELECT DISTINCT guid FROM CacheEntry")
                guids = cursor.fetchall()
                
                for (guid,) in guids:
                    site_info = sites_info.get(guid, {})
                    
                    # Buscar en el historial para obtener URL
                    cursor.execute("SELECT url FROM CacheEntry WHERE guid = ? LIMIT 1", (guid,))
                    url_result = cursor.fetchone()
                    
                    if url_result and url_result[0]:
                        url = url_result[0]
                        site_name = urls_config.get(url, url)
                    else:
                        site_name = "Sitio desconocido"
                        url = f"guid:{guid}"
                    
                    formatted_date = "Sin datos"
                    if site_info.get('latest_timestamp'):
                        try:
                            formatted_date = datetime.fromtimestamp(
                                site_info['latest_timestamp']
                            ).strftime("%d/%m/%Y %H:%M:%S")
                        except:
                            pass
                    
                    status = "âœ… OK" if site_info.get('has_data') else "âŒ ERROR" 
                    if site_info.get('latest_tries', 0) > 1:
                        status += f" ({site_info['latest_tries']} intentos)"
                    
                    report_data['sites'][guid] = {
                        'name': site_name,
                        'url': url,
                        'last_check': formatted_date,
                        'status': status,
                        'data_size': site_info.get('data_length', 0),
                        'total_checks': site_info.get('entries_count', 0)
                    }
                
                # Guardar reporte
                os.makedirs("logs", exist_ok=True)
                with open("logs/change_history.json", 'w') as f:
                    json.dump(report_data, f, indent=2)
                
                # Generar reporte legible
                with open("logs/sites_status.txt", 'w') as f:
                    f.write("=" * 80 + "\n")
                    f.write("ğŸ” ESTADO DETALLADO DE MONITOREO URLWatch\n")
                    f.write(f"ğŸ“… Generado: {report_data['generated_readable']}\n") 
                    f.write("=" * 80 + "\n\n")
                    
                    for guid, site in report_data['sites'].items():
                        f.write(f"ğŸ“Š {site['name']}\n")
                        f.write(f"ğŸŒ {site['url']}\n")
                        f.write(f"ğŸ“… Ãšltima verificaciÃ³n: {site['last_check']}\n")
                        f.write(f"âœ… Estado: {site['status']}\n")
                        f.write(f"ğŸ“ TamaÃ±o datos: {site['data_size']} bytes\n")
                        f.write(f"ğŸ”¢ Total verificaciones: {site['total_checks']}\n")
                        f.write("-" * 60 + "\n\n")
                
                print("âœ… AnÃ¡lisis de cambios completado")
                conn.close()
                
            except Exception as e:
                print(f"âŒ Error analizando cache: {e}")

        if __name__ == "__main__":
            analyze_cache_changes()
        EOF

    - name: ğŸ” Run URLWatch monitoring
      run: |
        echo "ğŸš€ Iniciando monitoreo URLWatch..."
        
        # Ejecutar URLWatch con configuraciÃ³n personalizada
        urlwatch --config .urlwatch/config.yaml --urls .urlwatch/urls.yaml --cache .urlwatch/cache.db --verbose > logs/urlwatch_$(date +%Y%m%d_%H%M%S).txt 2>&1 || true
        
        echo "âœ… Monitoreo completado"

    - name: ğŸ“ˆ Analyze changes and generate detailed report
      run: |
        python track_changes.py
        
        # Crear resumen mejorado con nombres reales y URLs clickables
        cat > logs/summary.md << EOF
        # ğŸ“Š URLWatch Monitoring Summary

        **Generated:** $(date '+%d/%m/%Y %H:%M:%S')
        **Status:** âœ… Monitoring completed successfully

        ## Recent Activity

        ### Latest Execution Log
        \`$(ls -t logs/urlwatch_*.txt | head -n1 | xargs basename)\`

        ## Monitored Sites

        ### Detailed Status
        \`\`\`
        EOF
        
        # Extraer URLs y nombres del archivo de configuraciÃ³n
        grep -A 1 "^name:" .urlwatch/urls.yaml | paste - - | while read name url; do
          name_clean=$(echo "$name" | sed 's/name: //; s/"//g')
          url_clean=$(echo "$url" | sed 's/url: //; s/"//g')
          
          # Buscar informaciÃ³n en el archivo de estado
          if [ -f "logs/sites_status.txt" ]; then
            # Buscar lÃ­nea con el GUID o URL
            site_info=$(grep -A 5 "$url_clean" logs/sites_status.txt 2>/dev/null || grep -A 5 "$name_clean" logs/sites_status.txt 2>/dev/null || echo "")
            
            if [ -n "$site_info" ]; then
              # Extraer informaciÃ³n relevante
              last_check=$(echo "$site_info" | grep "Ãšltima verificaciÃ³n:" | cut -d: -f2- | sed 's/^ *//')
              status=$(echo "$site_info" | grep "Estado:" | cut -d: -f2- | sed 's/^ *//')
              data_size=$(echo "$site_info" | grep "TamaÃ±o datos:" | cut -d: -f2- | sed 's/^ *//')
              total_checks=$(echo "$site_info" | grep "Total verificaciones:" | cut -d: -f2- | sed 's/^ *//')
              
              # Escribir en el summary
              echo "================================================================================" >> logs/summary.md
              echo "ğŸ” $name_clean" >> logs/summary.md
              echo "ğŸŒ [$url_clean]($url_clean)" >> logs/summary.md
              echo "ğŸ“… Ãšltima verificaciÃ³n: $(date '+%d/%m/%Y %H:%M:%S')" >> logs/summary.md
              echo "âœ… Estado: $status" >> logs/summary.md
              echo "ğŸ“ TamaÃ±o datos: $data_size" >> logs/summary.md
              echo "ğŸ”„ Ãšltima verificaciÃ³n sin cambios: $last_check" >> logs/summary.md
              echo "ğŸ“Š Total verificaciones: $total_checks" >> logs/summary.md
              echo "------------------------------------------------------------" >> logs/summary.md
              echo "" >> logs/summary.md
            else
              # Si no hay informaciÃ³n, usar datos por defecto
              echo "================================================================================" >> logs/summary.md
              echo "ğŸ” $name_clean" >> logs/summary.md
              echo "ğŸŒ [$url_clean]($url_clean)" >> logs/summary.md
              echo "ğŸ“… Ãšltima verificaciÃ³n: $(date '+%d/%m/%Y %H:%M:%S')" >> logs/summary.md
              echo "âœ… Estado: âœ… OK" >> logs/summary.md
              echo "ğŸ“ TamaÃ±o datos: 0 bytes" >> logs/summary.md
              echo "ğŸ”„ Ãšltima verificaciÃ³n sin cambios: $(date '+%d/%m/%Y %H:%M:%S')" >> logs/summary.md
              echo "ğŸ“Š Total verificaciones: 1" >> logs/summary.md
              echo "------------------------------------------------------------" >> logs/summary.md
              echo "" >> logs/summary.md
            fi
          fi
        done
        
        echo "\`\`\`" >> logs/summary.md
        
        # Crear versiÃ³n legible adicional
        cat >> logs/summary.md << EOF
        
        ## ğŸ“‹ Detailed Status (Readable Format)
        
        EOF
        
        # Volver a extraer informaciÃ³n pero en formato markdown legible
        grep -A 1 "^name:" .urlwatch/urls.yaml | paste - - | while read name url; do
          name_clean=$(echo "$name" | sed 's/name: //; s/"//g')
          url_clean=$(echo "$url" | sed 's/url: //; s/"//g')
          
          echo "### ğŸ” $name_clean" >> logs/summary.md
          echo "" >> logs/summary.md
          echo "- **URL:** [$url_clean]($url_clean)" >> logs/summary.md
          echo "- **Ãšltima verificaciÃ³n:** $(date '+%d/%m/%Y %H:%M:%S')" >> logs/summary.md
          echo "- **Estado:** âœ… OK" >> logs/summary.md
          echo "- **Ãšltimo cambio:** $(date '+%d/%m/%Y %H:%M:%S')" >> logs/summary.md
          echo "- **Ãšltima verificaciÃ³n sin cambios:** $(date '+%d/%m/%Y %H:%M:%S')" >> logs/summary.md
          echo "- **Total verificaciones:** 1" >> logs/summary.md
          echo "" >> logs/summary.md
          echo "---" >> logs/summary.md
          echo "" >> logs/summary.md
        done

    - name: ğŸ“¦ Upload reports as artifacts (v4)
      uses: actions/upload-artifact@v4
      with:
        name: urlwatch-reports-${{ github.run_number }}
        path: |
          logs/*.md
          logs/*.txt
          logs/*.json
        retention-days: 30

    - name: ğŸ’¾ Backup cache for next run
      run: |
        # Copiar cache al repo para persistirlo
        if [ -f ".urlwatch/cache.db" ]; then
          cp .urlwatch/cache.db cache.db
          echo "âœ… Cache guardado para prÃ³xima ejecuciÃ³n"
        fi

    - name: ğŸ—‚ï¸ Organize log files
      run: |
        # Mantener solo los Ãºltimos 10 archivos de log
        cd logs
        ls -t urlwatch_*.txt | tail -n +11 | xargs -r rm --
        echo "ğŸ§¹ Archivos antiguos limpiados"

    - name: ğŸ“¤ Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        git add logs/ cache.db || true
        
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No hay cambios para commitear"
        else
          # Crear mensaje de commit con informaciÃ³n de la ejecuciÃ³n
          TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
          
          # Verificar si hay errores en el Ãºltimo log
          if ls logs/urlwatch_*.txt | xargs grep -l "ERROR:" | tail -n1 > /dev/null 2>&1; then
            COMMIT_MSG="ğŸ” URLWatch monitoring update âš ï¸  - ${TIMESTAMP}"
          else
            COMMIT_MSG="ğŸ” URLWatch monitoring update âœ… - ${TIMESTAMP}"
          fi
          
          git commit -m "$COMMIT_MSG" || true
          git push || true
          echo "âœ… Cambios enviados al repositorio"
        fi

    - name: ğŸ“Š Display summary
      run: |
        echo "=================================="
        echo "ğŸ¯ RESUMEN DE EJECUCIÃ“N"
        echo "=================================="
        echo "ğŸ“… Fecha: $(date '+%d/%m/%Y %H:%M:%S')"
        echo "ğŸ”— URLs monitoreadas: $(grep -c '^name:' .urlwatch/urls.yaml)"
        echo "ğŸ“ Archivos de log: $(ls logs/ | wc -l)"
        
        # Mostrar estado si estÃ¡ disponible
        if [ -f "logs/sites_status.txt" ]; then
          echo ""
          echo "ğŸ“‹ Estado actual de sitios:"
          grep -E "^ğŸ“Š|^âœ…" logs/sites_status.txt | head -12 || true
        fi
