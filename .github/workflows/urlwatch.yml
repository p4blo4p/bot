name: ğŸ” URLWatch Monitoring - Detailed Tracking

on:
  schedule:
    # Ejecutar cada 6 horas
    - cron: '0 */6 * * *'
  workflow_dispatch: # Permitir ejecuciÃ³n manual
  
env:
  TZ: Europe/Madrid

jobs:
  monitor:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Necesario para el historial
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: ğŸ“¦ Install URLWatch and dependencies
      run: |
        pip install urlwatch
        pip install beautifulsoup4 lxml requests

    - name: ğŸ“ Create directories
      run: |
        mkdir -p logs
        mkdir -p .urlwatch

    - name: ğŸ”§ Setup URLWatch configuration
      run: |
        # Crear configuraciÃ³n detallada
        cat > .urlwatch/config.yaml << 'EOF'
        display:
          new: true
          error: true
          unchanged: false
          empty-diff: true
        
        report:
          text:
            line_length: 120
            details: true
            footer: true
            minimal: false
        
        # ConfiguraciÃ³n para mejor seguimiento
        storage:
          minidb:
            filename: .urlwatch/cache.db
            
        reporters:
          - text:
              filename: logs/detailed_report.txt
              details: true
        EOF

    - name: ğŸ“ Create URLs configuration  
      run: |
        cat > urls2watch.yaml << 'EOF'
        name: "Oposiciones UCA - InformÃ¡tica"
        url: "https://personal.uca.es/oposiciones-turno-libre/"
        timeout: 30
        filter:
          - html2text
          - grep: "informÃ¡tica"
          - strip
        ---
        name: "Cursos INAP - InformÃ¡tica"  
        url: "https://buscadorcursos.inap.es/#/?abierto=true&funcion=6"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|sistemas|tecnolog"
          - strip
        ---
        name: "Ayto Puerto Real - TablÃ³n"
        url: "https://puertoreal.sedelectronica.es/board" 
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|tÃ©cnico|sistemas"
          - strip
        ---
        name: "Ayto Puerto Real - OPE"
        url: "https://puertoreal.es/oferta-publica-de-empleo/"
        timeout: 30
        filter:
          - html2text  
          - grep: "(?i)informÃ¡tica|tÃ©cnico|sistemas"
          - strip
        ---
        # URLs corregidas (las anteriores daban 404)
        name: "BOE - BÃºsqueda Oposiciones"
        url: "https://www.boe.es/buscar/doc.php?coleccion=boe&modo=ultimosdias&tm=4"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|sistemas|tecnolog|tÃ©cnico superior|oposiciÃ³n"
          - strip
        ---
        name: "Junta de AndalucÃ­a - FunciÃ³n PÃºblica"
        url: "https://www.juntadeandalucia.es/organismos/funcionpublica/"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informÃ¡tica|sistemas|tecnolog|oposiciÃ³n"
          - strip
        EOF

    - name: ğŸ—‚ï¸ Load previous cache if exists
      run: |
        # Si existe cache anterior en el repo, restaurarlo
        if [ -f "cache.db" ]; then
          cp cache.db .urlwatch/cache.db
          echo "âœ… Cache anterior restaurado"
        else
          echo "â„¹ï¸ No hay cache anterior, iniciando desde cero"
        fi

    - name: ğŸ“Š Create change tracking script
      run: |
        cat > track_changes.py << 'EOF'
        #!/usr/bin/env python3
        import sqlite3
        import json
        import os
        from datetime import datetime
        from pathlib import Path

        def analyze_cache_changes():
            """Analiza la cache y genera reporte de cambios"""
            
            cache_file = Path(".urlwatch/cache.db")
            if not cache_file.exists():
                print("âŒ No se encontrÃ³ archivo de cache")
                return
            
            # Cargar informaciÃ³n previa si existe
            history_file = Path("logs/change_history.json")
            if history_file.exists():
                with open(history_file, 'r') as f:
                    history = json.load(f)
            else:
                history = {}
            
            # URLs y nombres para referencia
            url_names = {
                "https://personal.uca.es/oposiciones-turno-libre/": "Oposiciones UCA - InformÃ¡tica",
                "https://buscadorcursos.inap.es/#/?abierto=true&funcion=6": "Cursos INAP - InformÃ¡tica",
                "https://puertoreal.sedelectronica.es/board": "Ayto Puerto Real - TablÃ³n", 
                "https://puertoreal.es/oferta-publica-de-empleo/": "Ayto Puerto Real - OPE",
                "https://www.boe.es/buscar/doc.php?coleccion=boe&modo=ultimosdias&tm=4": "BOE - BÃºsqueda Oposiciones",
                "https://www.juntadeandalucia.es/organismos/funcionpublica/": "Junta de AndalucÃ­a - FunciÃ³n PÃºblica"
            }
            
            try:
                conn = sqlite3.connect(str(cache_file))
                cursor = conn.cursor()
                
                # Obtener todas las entradas ordenadas por timestamp
                cursor.execute("""
                    SELECT guid, timestamp, tries, etag, 
                           CASE WHEN data IS NULL THEN 0 ELSE 1 END as has_data,
                           length(data) as data_length
                    FROM CacheEntry 
                    ORDER BY timestamp DESC
                """)
                
                results = cursor.fetchall()
                current_time = datetime.now()
                
                # Procesar resultados
                sites_info = {}
                for row in results:
                    guid, timestamp, tries, etag, has_data, data_length = row
                    
                    if guid not in sites_info:
                        sites_info[guid] = {
                            'latest_timestamp': timestamp,
                            'latest_tries': tries,
                            'has_data': has_data,
                            'data_length': data_length or 0,
                            'entries_count': 0
                        }
                    
                    sites_info[guid]['entries_count'] += 1
                
                # Generar reporte detallado
                report_data = {
                    'generated_at': current_time.isoformat(),
                    'generated_readable': current_time.strftime("%d/%m/%Y %H:%M:%S"),
                    'sites': {}
                }
                
                # Por cada GUID, intentar encontrar la URL correspondiente
                cursor.execute("SELECT DISTINCT guid FROM CacheEntry")
                guids = cursor.fetchall()
                
                for (guid,) in guids:
                    site_info = sites_info.get(guid, {})
                    
                    # Buscar en historial previo para obtener URL
                    site_name = "Sitio desconocido"
                    site_url = f"guid:{guid}"
                    
                    # Intentar deducir desde nombres conocidos (esto es una aproximaciÃ³n)
                    # En una implementaciÃ³n real, guardarÃ­amos la URL en la cache
                    
                    formatted_date = "Sin datos"
                    if site_info.get('latest_timestamp'):
                        try:
                            formatted_date = datetime.fromtimestamp(
                                site_info['latest_timestamp']
                            ).strftime("%d/%m/%Y %H:%M:%S")
                        except:
                            pass
                    
                    status = "âœ… OK" if site_info.get('has_data') else "âŒ ERROR" 
                    if site_info.get('latest_tries', 0) > 1:
                        status += f" ({site_info['latest_tries']} intentos)"
                    
                    report_data['sites'][guid] = {
                        'name': site_name,
                        'url': site_url,
                        'last_check': formatted_date,
                        'status': status,
                        'data_size': site_info.get('data_length', 0),
                        'total_checks': site_info.get('entries_count', 0)
                    }
                
                # Guardar reporte
                os.makedirs("logs", exist_ok=True)
                with open("logs/change_history.json", 'w') as f:
                    json.dump(report_data, f, indent=2)
                
                # Generar reporte legible
                with open("logs/sites_status.txt", 'w') as f:
                    f.write("=" * 80 + "\n")
                    f.write("ğŸ” ESTADO DETALLADO DE MONITOREO URLWatch\n")
                    f.write(f"ğŸ“… Generado: {report_data['generated_readable']}\n") 
                    f.write("=" * 80 + "\n\n")
                    
                    for guid, site in report_data['sites'].items():
                        f.write(f"ğŸ“Š {site['name']}\n")
                        f.write(f"ğŸŒ {site['url']}\n")
                        f.write(f"ğŸ“… Ãšltima verificaciÃ³n: {site['last_check']}\n")
                        f.write(f"âœ… Estado: {site['status']}\n")
                        f.write(f"ğŸ“ TamaÃ±o datos: {site['data_size']} bytes\n")
                        f.write(f"ğŸ”¢ Total verificaciones: {site['total_checks']}\n")
                        f.write("-" * 60 + "\n\n")
                
                print("âœ… AnÃ¡lisis de cambios completado")
                conn.close()
                
            except Exception as e:
                print(f"âŒ Error analizando cache: {e}")

        if __name__ == "__main__":
            analyze_cache_changes()
        EOF

    - name: ğŸ” Run URLWatch monitoring
      run: |
        echo "ğŸš€ Iniciando monitoreo URLWatch..."
        
        # Ejecutar URLWatch con configuraciÃ³n personalizada
        urlwatch --config .urlwatch/config.yaml --urls urls2watch.yaml --cache .urlwatch/cache.db --verbose > logs/urlwatch_$(date +%Y%m%d_%H%M%S).txt 2>&1 || true
        
        echo "âœ… Monitoreo completado"

    - name: ğŸ“ˆ Analyze changes and generate detailed report
      run: |
        python track_changes.py
        
        # Crear resumen actualizado
        cat > logs/summary.md << EOF
        # URLWatch Monitoring Summary
        
        **Generated:** $(date '+%d/%m/%Y %H:%M:%S')
        **Status:** âœ… Monitoring completed successfully
        
        ## Recent Activity
        
        ### Latest Execution Log
        \`$(ls -t logs/urlwatch_*.txt | head -n1 | xargs basename)\`
        
        ### Monitored Sites
        EOF
        
        # Si existe el archivo de estado, agregarlo al resumen
        if [ -f "logs/sites_status.txt" ]; then
          echo "" >> logs/summary.md
          echo "### Detailed Status" >> logs/summary.md  
          echo "\`\`\`" >> logs/summary.md
          cat logs/sites_status.txt >> logs/summary.md
          echo "\`\`\`" >> logs/summary.md
        fi

    - name: ğŸ’¾ Backup cache for next run
      run: |
        # Copiar cache al repo para persistirlo
        if [ -f ".urlwatch/cache.db" ]; then
          cp .urlwatch/cache.db cache.db
          echo "âœ… Cache guardado para prÃ³xima ejecuciÃ³n"
        fi

    - name: ğŸ—‚ï¸ Organize log files
      run: |
        # Mantener solo los Ãºltimos 10 archivos de log
        cd logs
        ls -t urlwatch_*.txt | tail -n +11 | xargs -r rm --
        echo "ğŸ§¹ Archivos antiguos limpiados"

    - name: ğŸ“¤ Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        git add logs/ cache.db || true
        
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No hay cambios para commitear"
        else
          # Crear mensaje de commit con informaciÃ³n de la ejecuciÃ³n
          TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
          
          # Verificar si hay errores en el Ãºltimo log
          if ls logs/urlwatch_*.txt | xargs grep -l "ERROR:" | tail -n1 > /dev/null 2>&1; then
            COMMIT_MSG="ğŸ” URLWatch monitoring update âš ï¸  - ${TIMESTAMP}"
          else
            COMMIT_MSG="ğŸ” URLWatch monitoring update âœ… - ${TIMESTAMP}"
          fi
          
          git commit -m "$COMMIT_MSG" || true
          git push || true
          echo "âœ… Cambios enviados al repositorio"
        fi

    - name: ğŸ“Š Display summary
      run: |
        echo "=================================="
        echo "ğŸ¯ RESUMEN DE EJECUCIÃ“N"
        echo "=================================="
        echo "ğŸ“… Fecha: $(date '+%d/%m/%Y %H:%M:%S')"
        echo "ğŸ”— URLs monitoreadas: $(grep -c '^name:' urls2watch.yaml)"
        echo "ğŸ“ Archivos de log: $(ls logs/ | wc -l)"
        
        # Mostrar estado si estÃ¡ disponible
        if [ -f "logs/sites_status.txt" ]; then
          echo ""
          echo "ğŸ“‹ Estado actual de sitios:"
          grep -E "^ğŸ“Š|^âœ…" logs/sites_status.txt | head -12 || true
        fi
