name: 🔍 URLWatch Monitoring - Detailed Tracking

on:
  schedule:
    # Ejecutar cada 6 horas
    - cron: '0 */6 * * *'
  workflow_dispatch: # Permitir ejecución manual
  
env:
  TZ: Europe/Madrid

jobs:
  monitor:
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Necesario para el historial
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: 📦 Install URLWatch and dependencies
      run: |
        pip install urlwatch
        pip install beautifulsoup4 lxml requests

    - name: 📁 Create directories
      run: |
        mkdir -p logs
        mkdir -p .urlwatch

    - name: 🔧 Setup URLWatch configuration
      run: |
        # Crear configuración detallada
        cat > .urlwatch/config.yaml << 'EOF'
        display:
          new: true
          error: true
          unchanged: false
          empty-diff: true
        
        report:
          text:
            line_length: 120
            details: true
            footer: true
            minimal: false
        
        # Configuración para mejor seguimiento
        storage:
          minidb:
            filename: .urlwatch/cache.db
            
        reporters:
          - text:
              filename: logs/detailed_report.txt
              details: true
        EOF

    - name: 📝 Create URLs configuration  
      run: |
        cat > urls2watch.yaml << 'EOF'
        name: "Oposiciones UCA - Informática"
        url: "https://personal.uca.es/oposiciones-turno-libre/"
        timeout: 30
        filter:
          - html2text
          - grep: "informática"
          - strip
        ---
        name: "Cursos INAP - Informática"  
        url: "https://buscadorcursos.inap.es/#/?abierto=true&funcion=6"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informática|sistemas|tecnolog"
          - strip
        ---
        name: "Ayto Puerto Real - Tablón"
        url: "https://puertoreal.sedelectronica.es/board" 
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informática|técnico|sistemas"
          - strip
        ---
        name: "Ayto Puerto Real - OPE"
        url: "https://puertoreal.es/oferta-publica-de-empleo/"
        timeout: 30
        filter:
          - html2text  
          - grep: "(?i)informática|técnico|sistemas"
          - strip
        ---
        # URLs corregidas (las anteriores daban 404)
        name: "BOE - Búsqueda Oposiciones"
        url: "https://www.boe.es/buscar/doc.php?coleccion=boe&modo=ultimosdias&tm=4"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informática|sistemas|tecnolog|técnico superior|oposición"
          - strip
        ---
        name: "Junta de Andalucía - Función Pública"
        url: "https://www.juntadeandalucia.es/organismos/funcionpublica/"
        timeout: 30
        filter:
          - html2text
          - grep: "(?i)informática|sistemas|tecnolog|oposición"
          - strip
        EOF

    - name: 🗂️ Load previous cache if exists
      run: |
        # Si existe cache anterior en el repo, restaurarlo
        if [ -f "cache.db" ]; then
          cp cache.db .urlwatch/cache.db
          echo "✅ Cache anterior restaurado"
        else
          echo "ℹ️ No hay cache anterior, iniciando desde cero"
        fi

    - name: 📊 Create change tracking script
      run: |
        cat > track_changes.py << 'EOF'
        #!/usr/bin/env python3
        import sqlite3
        import json
        import os
        from datetime import datetime
        from pathlib import Path

        def analyze_cache_changes():
            """Analiza la cache y genera reporte de cambios"""
            
            cache_file = Path(".urlwatch/cache.db")
            if not cache_file.exists():
                print("❌ No se encontró archivo de cache")
                return
            
            # Cargar información previa si existe
            history_file = Path("logs/change_history.json")
            if history_file.exists():
                with open(history_file, 'r') as f:
                    history = json.load(f)
            else:
                history = {}
            
            # URLs y nombres para referencia
            url_names = {
                "https://personal.uca.es/oposiciones-turno-libre/": "Oposiciones UCA - Informática",
                "https://buscadorcursos.inap.es/#/?abierto=true&funcion=6": "Cursos INAP - Informática",
                "https://puertoreal.sedelectronica.es/board": "Ayto Puerto Real - Tablón", 
                "https://puertoreal.es/oferta-publica-de-empleo/": "Ayto Puerto Real - OPE",
                "https://www.boe.es/buscar/doc.php?coleccion=boe&modo=ultimosdias&tm=4": "BOE - Búsqueda Oposiciones",
                "https://www.juntadeandalucia.es/organismos/funcionpublica/": "Junta de Andalucía - Función Pública"
            }
            
            try:
                conn = sqlite3.connect(str(cache_file))
                cursor = conn.cursor()
                
                # Obtener todas las entradas ordenadas por timestamp
                cursor.execute("""
                    SELECT guid, timestamp, tries, etag, 
                           CASE WHEN data IS NULL THEN 0 ELSE 1 END as has_data,
                           length(data) as data_length
                    FROM CacheEntry 
                    ORDER BY timestamp DESC
                """)
                
                results = cursor.fetchall()
                current_time = datetime.now()
                
                # Procesar resultados
                sites_info = {}
                for row in results:
                    guid, timestamp, tries, etag, has_data, data_length = row
                    
                    if guid not in sites_info:
                        sites_info[guid] = {
                            'latest_timestamp': timestamp,
                            'latest_tries': tries,
                            'has_data': has_data,
                            'data_length': data_length or 0,
                            'entries_count': 0
                        }
                    
                    sites_info[guid]['entries_count'] += 1
                
                # Generar reporte detallado
                report_data = {
                    'generated_at': current_time.isoformat(),
                    'generated_readable': current_time.strftime("%d/%m/%Y %H:%M:%S"),
                    'sites': {}
                }
                
                # Por cada GUID, intentar encontrar la URL correspondiente
                cursor.execute("SELECT DISTINCT guid FROM CacheEntry")
                guids = cursor.fetchall()
                
                for (guid,) in guids:
                    site_info = sites_info.get(guid, {})
                    
                    # Buscar en historial previo para obtener URL
                    site_name = "Sitio desconocido"
                    site_url = f"guid:{guid}"
                    
                    # Intentar deducir desde nombres conocidos (esto es una aproximación)
                    # En una implementación real, guardaríamos la URL en la cache
                    
                    formatted_date = "Sin datos"
                    if site_info.get('latest_timestamp'):
                        try:
                            formatted_date = datetime.fromtimestamp(
                                site_info['latest_timestamp']
                            ).strftime("%d/%m/%Y %H:%M:%S")
                        except:
                            pass
                    
                    status = "✅ OK" if site_info.get('has_data') else "❌ ERROR" 
                    if site_info.get('latest_tries', 0) > 1:
                        status += f" ({site_info['latest_tries']} intentos)"
                    
                    report_data['sites'][guid] = {
                        'name': site_name,
                        'url': site_url,
                        'last_check': formatted_date,
                        'status': status,
                        'data_size': site_info.get('data_length', 0),
                        'total_checks': site_info.get('entries_count', 0)
                    }
                
                # Guardar reporte
                os.makedirs("logs", exist_ok=True)
                with open("logs/change_history.json", 'w') as f:
                    json.dump(report_data, f, indent=2)
                
                # Generar reporte legible
                with open("logs/sites_status.txt", 'w') as f:
                    f.write("=" * 80 + "\n")
                    f.write("🔍 ESTADO DETALLADO DE MONITOREO URLWatch\n")
                    f.write(f"📅 Generado: {report_data['generated_readable']}\n") 
                    f.write("=" * 80 + "\n\n")
                    
                    for guid, site in report_data['sites'].items():
                        f.write(f"📊 {site['name']}\n")
                        f.write(f"🌐 {site['url']}\n")
                        f.write(f"📅 Última verificación: {site['last_check']}\n")
                        f.write(f"✅ Estado: {site['status']}\n")
                        f.write(f"📏 Tamaño datos: {site['data_size']} bytes\n")
                        f.write(f"🔢 Total verificaciones: {site['total_checks']}\n")
                        f.write("-" * 60 + "\n\n")
                
                print("✅ Análisis de cambios completado")
                conn.close()
                
            except Exception as e:
                print(f"❌ Error analizando cache: {e}")

        if __name__ == "__main__":
            analyze_cache_changes()
        EOF

    - name: 🔍 Run URLWatch monitoring
      run: |
        echo "🚀 Iniciando monitoreo URLWatch..."
        
        # Ejecutar URLWatch con configuración personalizada
        urlwatch --config .urlwatch/config.yaml --urls urls2watch.yaml --cache .urlwatch/cache.db --verbose > logs/urlwatch_$(date +%Y%m%d_%H%M%S).txt 2>&1 || true
        
        echo "✅ Monitoreo completado"


          echo "" >> logs/summary.md
          echo "### Detailed Status" >> logs/summary.md  
          echo "\`\`\`" >> logs/summary.md
          cat logs/sites_status.txt >> logs/summary.md
          echo "\`\`\`" >> logs/summary.md
        fi

    - name: 💾 Backup cache for next run
      run: |
        # Copiar cache al repo para persistirlo
        if [ -f ".urlwatch/cache.db" ]; then
          cp .urlwatch/cache.db cache.db
          echo "✅ Cache guardado para próxima ejecución"
        fi

    - name: 🗂️ Organize log files
      run: |
        # Mantener solo los últimos 10 archivos de log
        cd logs
        ls -t urlwatch_*.txt | tail -n +11 | xargs -r rm --
        echo "🧹 Archivos antiguos limpiados"

    - name: 📤 Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        git add logs/ cache.db || true
        
        if git diff --staged --quiet; then
          echo "ℹ️ No hay cambios para commitear"
        else
          # Crear mensaje de commit con información de la ejecución
          TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
          
          # Verificar si hay errores en el último log
          if ls logs/urlwatch_*.txt | xargs grep -l "ERROR:" | tail -n1 > /dev/null 2>&1; then
            COMMIT_MSG="🔍 URLWatch monitoring update ⚠️  - ${TIMESTAMP}"
          else
            COMMIT_MSG="🔍 URLWatch monitoring update ✅ - ${TIMESTAMP}"
          fi
          
          git commit -m "$COMMIT_MSG" || true
          git push || true
          echo "✅ Cambios enviados al repositorio"
        fi

    - name: 📊 Display summary
      run: |
        echo "=================================="
        echo "🎯 RESUMEN DE EJECUCIÓN"
        echo "=================================="
        echo "📅 Fecha: $(date '+%d/%m/%Y %H:%M:%S')"
        echo "🔗 URLs monitoreadas: $(grep -c '^name:' urls2watch.yaml)"
        echo "📁 Archivos de log: $(ls logs/ | wc -l)"
        
        # Mostrar estado si está disponible
        if [ -f "logs/sites_status.txt" ]; then
          echo ""
          echo "📋 Estado actual de sitios:"
          grep -E "^📊|^✅" logs/sites_status.txt | head -12 || true
        fi
